{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Terrorist Attacks\n",
    "## Objective\n",
    "To build a classifier that can predict the group responsible for individual terrorist attacks around the world.\n",
    "## Introduction\n",
    "The classifier is trained using the [Global Terrorism Database (GTD)](http://apps.start.umd.edu/gtd/downloads/dataset/Codebook.pdf). The dataset consists of nearly 200,000 terrorist attacks including bombings, assassinations, and kidnappings that have occured since 1970. Each of these attacks includes information on over 45 variables including location, type of weapon used, and nationality of the perpetrator.\n",
    "\n",
    "This classifer is focused on using information about an attack to predict the responsible group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "Before training, the data must be prepared by selecting certain features to remove or augment. The following features from the dataset are kept (reference the [GTD](http://apps.start.umd.edu/gtd/downloads/dataset/Codebook.pdf) for more information on each feature):\n",
    "- <b>Numerical Features (Bucketized)</b><br/>\n",
    "nkill, nkillus, nkillter, nwound, nwoundus, nperps\n",
    "\n",
    "- <b>Categorical Features</b><br/>\n",
    "crit1, crit2, crit3, attacktype1, attacktype2, attacktype3, weaptype1, weapsubtype1, weaptype2, weapsubtype2, weaptype3, weapsubtype3, weaptype4, weapsubtype4, natlty1, natlty2, natlty3, targtype1, targtype2, targtype3, claimed, doubtterr, country, multiple, success, suicide<br/>\n",
    "\n",
    "- <b>Label</b><br/>\n",
    "gnome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make necessary imports\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global variables\n",
    "columns_to_use = ['country', 'crit1', 'crit2', 'crit3', 'doubtterr', 'multiple', 'success', 'suicide', 'attacktype1', 'attacktype2', 'attacktype3', 'targtype1', 'natlty1', 'targtype2', 'natlty2', 'targtype3', 'natlty3', 'nperps', 'claimed', 'weaptype1', 'weapsubtype1', 'weaptype2', 'weapsubtype2', 'weaptype3', 'weapsubtype3', 'weaptype4', 'weapsubtype4', 'nkill', 'nkillus', 'nkillter', 'nwound', 'nwoundus', 'gname']\n",
    "input_file = 'data/gtd1993_0617dist.xlsx'\n",
    "csv_file = 'data/gtd1993_0617dist.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the excel data file to a csv\n",
    "wb = xlrd.open_workbook(input_file)\n",
    "sh = wb.sheets()[0]\n",
    "output_csv = open(csv_file, 'w')\n",
    "wr = csv.writer(output_csv, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "for rownum in range(sh.nrows):\n",
    "    wr.writerow(sh.row_values(rownum))\n",
    "\n",
    "output_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Deep Neural Network\n",
    "Now the neural network must be constructed. It is a deep neural network that uses a mix of numerical, bucketized columns and categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the numerical columns\n",
    "num_killed = tf.feature_column.numeric_column(key='nkill')\n",
    "num_wounded = tf.feature_column.numeric_column(key='nwound')\n",
    "num_us_killed = tf.feature_column.numeric_column(key='nkillus')\n",
    "num_us_wounded = tf.feature_column.numeric_column(key='nwoundus')\n",
    "num_perps = tf.feature_column.numeric_column(key='nperps')\n",
    "num_perps_killed = tf.feature_column.numeric_column(key='nkillter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucketize each of the numerical columns\n",
    "num_killed = tf.feature_column.bucketized_column(source_column=num_killed, boundaries=[5, 25, 50])\n",
    "num_wounded = tf.feature_column.bucketized_column(source_column=num_wounded, boundaries=[5, 25, 50])\n",
    "num_us_killed = tf.feature_column.bucketized_column(source_column=num_us_killed, boundaries=[5, 25, 50])\n",
    "num_us_wounded = tf.feature_column.bucketized_column(source_column=num_us_wounded, boundaries=[5, 25, 50])\n",
    "num_perps = tf.feature_column.bucketized_column(source_column=num_perps, boundaries=[1, 3])\n",
    "num_perp_killed = tf.feature_column.bucketized_column(source_column=num_perps_killed, boundaries=[1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the categorical columns\n",
    "terror_criteria1 = tf.feature_column.categorical_column_with_identity(key='crit1', num_buckets=2)\n",
    "terror_criteria2 = tf.feature_column.categorical_column_with_identity(key='crit2', num_buckets=2)\n",
    "terror_criteria3 = tf.feature_column.categorical_column_with_identity(key='crit3', num_buckets=2)\n",
    "attack_type1 = tf.feature_column.categorical_column_with_identity(key='attacktype1', num_buckets=9)\n",
    "attack_type2 = tf.feature_column.categorical_column_with_identity(key='attacktype2', num_buckets=9)\n",
    "attack_type3 = tf.feature_column.categorical_column_with_identity(key='attacktype3', num_buckets=9)\n",
    "weapon_type1 = tf.feature_column.categorical_column_with_identity(key='weaptype1', num_buckets=13)\n",
    "weapon_subtype1 = tf.feature_column.categorical_column_with_identity(key='weapsubtype1', num_buckets=30)\n",
    "weapon_type2 = tf.feature_column.categorical_column_with_identity(key='weaptype2', num_buckets=13)\n",
    "weapon_subtype2 = tf.feature_column.categorical_column_with_identity(key='weapsubtype2', num_buckets=30)\n",
    "weapon_type3 = tf.feature_column.categorical_column_with_identity(key='weaptype3', num_buckets=13)\n",
    "weapon_subtype3 = tf.feature_column.categorical_column_with_identity(key='weapsubtype3', num_buckets=30)\n",
    "weapon_type4 = tf.feature_column.categorical_column_with_identity(key='weaptype4', num_buckets=13)\n",
    "weapon_subtype4 = tf.feature_column.categorical_column_with_identity(key='weapsubtype4', num_buckets=30)\n",
    "target_nationality1 = tf.feature_column.categorical_column_with_identity(key='natlty1', num_buckets=10004)\n",
    "target_nationality2 = tf.feature_column.categorical_column_with_identity(key='natlty2', num_buckets=10004)\n",
    "target_nationality3 = tf.feature_column.categorical_column_with_identity(key='natlty3', num_buckets=10004)\n",
    "target_type1 = tf.feature_column.categorical_column_with_identity(key='targtype1', num_buckets=22)\n",
    "target_type2 = tf.feature_column.categorical_column_with_identity(key='targtype2', num_buckets=22)\n",
    "target_type3 = tf.feature_column.categorical_column_with_identity(key='targtype3', num_buckets=22)\n",
    "responsibility_claimed = tf.feature_column.categorical_column_with_identity(key='claimed', num_buckets=2)\n",
    "terrorism_doubt = tf.feature_column.categorical_column_with_identity(key='doubtterr', num_buckets=2)\n",
    "country_occurred = tf.feature_column.categorical_column_with_identity(key='country', num_buckets=1004)\n",
    "multiple_incidents = tf.feature_column.categorical_column_with_identity(key='multiple', num_buckets=2)\n",
    "was_successful = tf.feature_column.categorical_column_with_identity(key='success', num_buckets=2)\n",
    "was_suicide = tf.feature_column.categorical_column_with_identity(key='suicide', num_buckets=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert each of the categorical columns to indicator functions\n",
    "terror_criteria1 = tf.feature_column.indicator_column(terror_criteria1)\n",
    "terror_criteria2 = tf.feature_column.indicator_column(terror_criteria2)\n",
    "terror_criteria3 = tf.feature_column.indicator_column(terror_criteria3)\n",
    "attack_type1 = tf.feature_column.indicator_column(attack_type1)\n",
    "attack_type2 = tf.feature_column.indicator_column(attack_type2)\n",
    "attack_type3 = tf.feature_column.indicator_column(attack_type3)\n",
    "weapon_type1 = tf.feature_column.indicator_column(weapon_type1)\n",
    "weapon_subtype1 = tf.feature_column.indicator_column(weapon_subtype1)\n",
    "weapon_type2 = tf.feature_column.indicator_column(weapon_type2)\n",
    "weapon_subtype2 = tf.feature_column.indicator_column(weapon_subtype2)\n",
    "weapon_type3 = tf.feature_column.indicator_column(weapon_type3)\n",
    "weapon_subtype3 = tf.feature_column.indicator_column(weapon_subtype3)\n",
    "weapon_type4 = tf.feature_column.indicator_column(weapon_type4)\n",
    "weapon_subtype4 = tf.feature_column.indicator_column(weapon_subtype4)\n",
    "target_nationality1 = tf.feature_column.indicator_column(target_nationality1)\n",
    "target_nationality2 = tf.feature_column.indicator_column(target_nationality2)\n",
    "target_nationality3 = tf.feature_column.indicator_column(target_nationality3)\n",
    "target_type1 = tf.feature_column.indicator_column(target_type1)\n",
    "target_type2 = tf.feature_column.indicator_column(target_type2)\n",
    "target_type3 = tf.feature_column.indicator_column(target_type3)\n",
    "responsibility_claimed = tf.feature_column.indicator_column(responsibility_claimed)\n",
    "terrorism_doubt = tf.feature_column.indicator_column(terrorism_doubt)\n",
    "country_occurred = tf.feature_column.indicator_column(country_occurred)\n",
    "multiple_incidents = tf.feature_column.indicator_column(multiple_incidents)\n",
    "was_successful = tf.feature_column.indicator_column(was_successful)\n",
    "was_suicide = tf.feature_column.indicator_column(was_suicide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a list of each of the deep columns\n",
    "deep_columns = [\n",
    "    num_killed,\n",
    "    num_us_killed,\n",
    "    num_us_wounded,\n",
    "    num_perps,\n",
    "    num_perp_killed,\n",
    "    terror_criteria1,\n",
    "    terror_criteria2,\n",
    "    terror_criteria3,\n",
    "    attack_type1,\n",
    "    attack_type2,\n",
    "    attack_type3,\n",
    "    weapon_type1,\n",
    "    weapon_subtype1,\n",
    "    weapon_type2,\n",
    "    weapon_subtype2,\n",
    "    weapon_type3,\n",
    "    weapon_subtype3,\n",
    "    weapon_type4,\n",
    "    weapon_subtype4,\n",
    "    target_nationality1,\n",
    "    target_nationality2,\n",
    "    target_nationality3,\n",
    "    target_type1,\n",
    "    target_type2,\n",
    "    target_type3,\n",
    "    responsibility_claimed,\n",
    "    terrorism_doubt,\n",
    "    country_occurred,\n",
    "    multiple_incidents,\n",
    "    was_successful,\n",
    "    was_suicide\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/xm/l9l7xyqx2qs1f_7yfkshv9v40000gn/T/tmpyx_9221j\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/xm/l9l7xyqx2qs1f_7yfkshv9v40000gn/T/tmpyx_9221j', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11fe7b518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# initializing the DNN\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "    n_classes=100,\n",
    "    feature_columns=deep_columns,\n",
    "    hidden_units=[100, 50],\n",
    "    activation_fn=\"relu\",\n",
    "    dropout=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "Now the network must be trained on the GTD data from 1993."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to load in the training data\n",
    "def load_data(label_name):\n",
    "    train = pd.read_csv(csv_file, usecols=columns_to_use)\n",
    "    train_x, train_y = train, train.pop(label_name)\n",
    "    \n",
    "    # replace all NaN values with -1\n",
    "    train_x.fillna(-1, inplace=True)\n",
    "    \n",
    "    # convert the features into a tensor\n",
    "    train_final_x = {}\n",
    "    for key in train_x:\n",
    "        train_final_x[key] = tf.convert_to_tensor(train_x[key].astype(int))\n",
    "    \n",
    "    # return the features and labels as tensors\n",
    "    return train_final_x, tf.convert_to_tensor(train_y)\n",
    "    \n",
    "training_data = load_data(label_name='gname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f207c4e0961e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train the network on the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 824\u001b[0;31m           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warm_start_settings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/canned/dnn.py\u001b[0m in \u001b[0;36m_model_fn\u001b[0;34m(features, labels, mode, config)\u001b[0m\n\u001b[1;32m    352\u001b[0m           \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m           \u001b[0minput_layer_partitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_layer_partitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m           config=config)\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     super(DNNClassifier, self).__init__(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/canned/dnn.py\u001b[0m in \u001b[0;36m_dnn_model_fn\u001b[0;34m(features, labels, mode, head, hidden_units, feature_columns, optimizer, activation_fn, dropout, input_layer_partitioner, config)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         input_layer_partitioner=input_layer_partitioner)\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train_op_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/canned/dnn.py\u001b[0m in \u001b[0;36mdnn_logit_fn\u001b[0;34m(features, mode)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglorot_uniform_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             name=hidden_layer_scope)\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m           \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/layers/core.py\u001b[0m in \u001b[0;36mdense\u001b[0;34m(inputs, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0m_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 _reuse=reuse)\n\u001b[0;32m--> 253\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \"\"\"\n\u001b[0;32m--> 825\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m   def _add_inbound_node(self,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m             raise ValueError('A layer\\'s `call` method should return a Tensor '\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    165\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "# train the network on the training set\n",
    "dnn.train(input_fn=lambda: training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- K-fold splitting of the data\n",
    "- Testing out which epoch and batch sizes are optimal\n",
    "- Graph accuracy over epoch count of training set and test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
