{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Terrorist Attacks\n",
    "## Objective\n",
    "To build a classifier that can predict the group responsible for individual terrorist attacks around the world.\n",
    "## Introduction\n",
    "The classifier is trained using the [Global Terrorism Database (GTD)](http://apps.start.umd.edu/gtd/downloads/dataset/Codebook.pdf). The dataset consists of nearly 200,000 terrorist attacks including bombings, assassinations, and kidnappings that have occured since 1970. Each of these attacks includes information on over 45 variables including location, type of weapon used, and nationality of the perpetrator.\n",
    "\n",
    "This classifer is focused on using information about an attack to predict the responsible group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "Before training, the data must be prepared by selecting certain features to remove or augment. The following features from the dataset are kept (reference the [GTD](http://apps.start.umd.edu/gtd/downloads/dataset/Codebook.pdf) for more information on each feature):\n",
    "- <b>Numerical Features (Bucketized)</b><br/>\n",
    "nkill, nkillus, nkillter, nwound, nwoundus, nperps\n",
    "\n",
    "- <b>Categorical Features</b><br/>\n",
    "crit1, crit2, crit3, attacktype1, attacktype2, attacktype3, weaptype1, weapsubtype1, weaptype2, weapsubtype2, weaptype3, weapsubtype3, weaptype4, weapsubtype4, natlty1, natlty2, natlty3, targtype1, targtype2, targtype3, claimed, doubtterr, country, multiple, success, suicide<br/>\n",
    "\n",
    "- <b>Label</b><br/>\n",
    "gnome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make necessary imports\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global variables\n",
    "columns_to_use = ['country', 'crit1', 'crit2', 'crit3', 'doubtterr', 'multiple', 'success', 'suicide', 'attacktype1', 'attacktype2', 'attacktype3', 'targtype1', 'natlty1', 'targtype2', 'natlty2', 'targtype3', 'natlty3', 'nperps', 'claimed', 'weaptype1', 'weapsubtype1', 'weaptype2', 'weapsubtype2', 'weaptype3', 'weapsubtype3', 'weaptype4', 'weapsubtype4', 'nkill', 'nkillus', 'nkillter', 'nwound', 'nwoundus', 'gname']\n",
    "input_file = 'data/gtd1993_0617dist.xlsx'\n",
    "csv_file = 'data/gtd1993_0617dist.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the excel data file to a csv\n",
    "wb = xlrd.open_workbook(input_file)\n",
    "sh = wb.sheets()[0]\n",
    "output_csv = open(csv_file, 'w')\n",
    "wr = csv.writer(output_csv, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "for rownum in range(sh.nrows):\n",
    "    wr.writerow(sh.row_values(rownum))\n",
    "\n",
    "output_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Deep Neural Network\n",
    "Now the neural network must be constructed. It is a deep neural network that uses a mix of numerical, bucketized columns and categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the numerical columns\n",
    "num_killed = tf.feature_column.numeric_column(key='nkill')\n",
    "num_wounded = tf.feature_column.numeric_column(key='nwound')\n",
    "num_us_killed = tf.feature_column.numeric_column(key='nkillus')\n",
    "num_us_wounded = tf.feature_column.numeric_column(key='nwoundus')\n",
    "num_perps = tf.feature_column.numeric_column(key='nperps')\n",
    "num_perps_killed = tf.feature_column.numeric_column(key='nkillter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucketize each of the numerical columns\n",
    "num_killed = tf.feature_column.bucketized_column(source_column=num_killed, boundaries=[5, 25, 50])\n",
    "num_wounded = tf.feature_column.bucketized_column(source_column=num_wounded, boundaries=[5, 25, 50])\n",
    "num_us_killed = tf.feature_column.bucketized_column(source_column=num_us_killed, boundaries=[5, 25, 50])\n",
    "num_us_wounded = tf.feature_column.bucketized_column(source_column=num_us_wounded, boundaries=[5, 25, 50])\n",
    "num_perps = tf.feature_column.bucketized_column(source_column=num_perps, boundaries=[1, 3])\n",
    "num_perp_killed = tf.feature_column.bucketized_column(source_column=num_perps_killed, boundaries=[1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the categorical columns\n",
    "terror_criteria1 = tf.feature_column.categorical_column_with_identity(key='crit1', num_buckets=2)\n",
    "terror_criteria2 = tf.feature_column.categorical_column_with_identity(key='crit2', num_buckets=2)\n",
    "terror_criteria3 = tf.feature_column.categorical_column_with_identity(key='crit3', num_buckets=2)\n",
    "attack_type1 = tf.feature_column.categorical_column_with_identity(key='attacktype1', num_buckets=9)\n",
    "attack_type2 = tf.feature_column.categorical_column_with_identity(key='attacktype2', num_buckets=9)\n",
    "attack_type3 = tf.feature_column.categorical_column_with_identity(key='attacktype3', num_buckets=9)\n",
    "weapon_type1 = tf.feature_column.categorical_column_with_identity(key='weaptype1', num_buckets=13)\n",
    "weapon_subtype1 = tf.feature_column.categorical_column_with_identity(key='weapsubtype1', num_buckets=30)\n",
    "weapon_type2 = tf.feature_column.categorical_column_with_identity(key='weaptype2', num_buckets=13)\n",
    "weapon_subtype2 = tf.feature_column.categorical_column_with_identity(key='weapsubtype2', num_buckets=30)\n",
    "weapon_type3 = tf.feature_column.categorical_column_with_identity(key='weaptype3', num_buckets=13)\n",
    "weapon_subtype3 = tf.feature_column.categorical_column_with_identity(key='weapsubtype3', num_buckets=30)\n",
    "weapon_type4 = tf.feature_column.categorical_column_with_identity(key='weaptype4', num_buckets=13)\n",
    "weapon_subtype4 = tf.feature_column.categorical_column_with_identity(key='weapsubtype4', num_buckets=30)\n",
    "target_nationality1 = tf.feature_column.categorical_column_with_identity(key='natlty1', num_buckets=10004)\n",
    "target_nationality2 = tf.feature_column.categorical_column_with_identity(key='natlty2', num_buckets=10004)\n",
    "target_nationality3 = tf.feature_column.categorical_column_with_identity(key='natlty3', num_buckets=10004)\n",
    "target_type1 = tf.feature_column.categorical_column_with_identity(key='targtype1', num_buckets=22)\n",
    "target_type2 = tf.feature_column.categorical_column_with_identity(key='targtype2', num_buckets=22)\n",
    "target_type3 = tf.feature_column.categorical_column_with_identity(key='targtype3', num_buckets=22)\n",
    "responsibility_claimed = tf.feature_column.categorical_column_with_identity(key='claimed', num_buckets=2)\n",
    "terrorism_doubt = tf.feature_column.categorical_column_with_identity(key='doubtterr', num_buckets=2)\n",
    "country_occurred = tf.feature_column.categorical_column_with_identity(key='country', num_buckets=1004)\n",
    "multiple_incidents = tf.feature_column.categorical_column_with_identity(key='multiple', num_buckets=2)\n",
    "was_successful = tf.feature_column.categorical_column_with_identity(key='success', num_buckets=2)\n",
    "was_suicide = tf.feature_column.categorical_column_with_identity(key='suicide', num_buckets=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert each of the categorical columns to indicator functions\n",
    "terror_criteria1 = tf.feature_column.indicator_column(terror_criteria1)\n",
    "terror_criteria2 = tf.feature_column.indicator_column(terror_criteria2)\n",
    "terror_criteria3 = tf.feature_column.indicator_column(terror_criteria3)\n",
    "attack_type1 = tf.feature_column.indicator_column(attack_type1)\n",
    "attack_type2 = tf.feature_column.indicator_column(attack_type2)\n",
    "attack_type3 = tf.feature_column.indicator_column(attack_type3)\n",
    "weapon_type1 = tf.feature_column.indicator_column(weapon_type1)\n",
    "weapon_subtype1 = tf.feature_column.indicator_column(weapon_subtype1)\n",
    "weapon_type2 = tf.feature_column.indicator_column(weapon_type2)\n",
    "weapon_subtype2 = tf.feature_column.indicator_column(weapon_subtype2)\n",
    "weapon_type3 = tf.feature_column.indicator_column(weapon_type3)\n",
    "weapon_subtype3 = tf.feature_column.indicator_column(weapon_subtype3)\n",
    "weapon_type4 = tf.feature_column.indicator_column(weapon_type4)\n",
    "weapon_subtype4 = tf.feature_column.indicator_column(weapon_subtype4)\n",
    "target_nationality1 = tf.feature_column.indicator_column(target_nationality1)\n",
    "target_nationality2 = tf.feature_column.indicator_column(target_nationality2)\n",
    "target_nationality3 = tf.feature_column.indicator_column(target_nationality3)\n",
    "target_type1 = tf.feature_column.indicator_column(target_type1)\n",
    "target_type2 = tf.feature_column.indicator_column(target_type2)\n",
    "target_type3 = tf.feature_column.indicator_column(target_type3)\n",
    "responsibility_claimed = tf.feature_column.indicator_column(responsibility_claimed)\n",
    "terrorism_doubt = tf.feature_column.indicator_column(terrorism_doubt)\n",
    "country_occurred = tf.feature_column.indicator_column(country_occurred)\n",
    "multiple_incidents = tf.feature_column.indicator_column(multiple_incidents)\n",
    "was_successful = tf.feature_column.indicator_column(was_successful)\n",
    "was_suicide = tf.feature_column.indicator_column(was_suicide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a list of each of the deep columns\n",
    "deep_columns = [\n",
    "    num_killed,\n",
    "    num_us_killed,\n",
    "    num_us_wounded,\n",
    "    num_perps,\n",
    "    num_perp_killed,\n",
    "    terror_criteria1,\n",
    "    terror_criteria2,\n",
    "    terror_criteria3,\n",
    "    attack_type1,\n",
    "    attack_type2,\n",
    "    attack_type3,\n",
    "    weapon_type1,\n",
    "    weapon_subtype1,\n",
    "    weapon_type2,\n",
    "    weapon_subtype2,\n",
    "    weapon_type3,\n",
    "    weapon_subtype3,\n",
    "    weapon_type4,\n",
    "    weapon_subtype4,\n",
    "    target_nationality1,\n",
    "    target_nationality2,\n",
    "    target_nationality3,\n",
    "    target_type1,\n",
    "    target_type2,\n",
    "    target_type3,\n",
    "    responsibility_claimed,\n",
    "    terrorism_doubt,\n",
    "    country_occurred,\n",
    "    multiple_incidents,\n",
    "    was_successful,\n",
    "    was_suicide\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/xm/l9l7xyqx2qs1f_7yfkshv9v40000gn/T/tmp_sao7os5\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/xm/l9l7xyqx2qs1f_7yfkshv9v40000gn/T/tmp_sao7os5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x120f8f0b8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# initializing the DNN\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "    n_classes=100,\n",
    "    feature_columns=deep_columns,\n",
    "    hidden_units=[100, 50],\n",
    "    activation_fn=tf.nn.relu,\n",
    "    dropout=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "Now the network must be trained on the GTD data from 1993."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to load in the training data\n",
    "def load_data(label_name):\n",
    "    train = pd.read_csv(csv_file, usecols=columns_to_use)\n",
    "    train_x, train_y = train, train.pop(label_name)\n",
    "    \n",
    "    # replace all NaN values with -1\n",
    "    train_x.fillna(-1, inplace=True)\n",
    "    \n",
    "    # convert the features into a tensor\n",
    "    train_final_x = {}\n",
    "    for key in train_x:\n",
    "        train_final_x[key] = tf.convert_to_tensor(train_x[key].astype(int))\n",
    "    \n",
    "    # return the features and labels as tensors\n",
    "    return train_final_x, tf.convert_to_tensor(train_y)\n",
    "    \n",
    "training_data = load_data(label_name='gname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Labels dtype should be integer. Instead got <dtype: 'string'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f207c4e0961e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train the network on the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 824\u001b[0;31m           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warm_start_settings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/canned/dnn.py\u001b[0m in \u001b[0;36m_model_fn\u001b[0;34m(features, labels, mode, config)\u001b[0m\n\u001b[1;32m    352\u001b[0m           \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m           \u001b[0minput_layer_partitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_layer_partitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m           config=config)\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     super(DNNClassifier, self).__init__(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/canned/dnn.py\u001b[0m in \u001b[0;36m_dnn_model_fn\u001b[0;34m(features, labels, mode, head, hidden_units, feature_columns, optimizer, activation_fn, dropout, input_layer_partitioner, config)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mtrain_op_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_train_op_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         logits=logits)\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/canned/head.py\u001b[0m in \u001b[0;36mcreate_estimator_spec\u001b[0;34m(self, features, mode, logits, labels, train_op_fn, regularization_losses)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m       training_loss, unreduced_loss, weights, label_ids = self.create_loss(\n\u001b[0;32m--> 761\u001b[0;31m           features=features, mode=mode, logits=logits, labels=labels)\n\u001b[0m\u001b[1;32m    762\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mregularization_losses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mregularization_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregularization_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/canned/head.py\u001b[0m in \u001b[0;36mcreate_loss\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    674\u001b[0m     labels = _check_dense_labels_match_logits_and_reshape(\n\u001b[1;32m    675\u001b[0m         labels=labels, logits=logits, expected_labels_dimension=1)\n\u001b[0;32m--> 676\u001b[0;31m     \u001b[0mlabel_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_label_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m       unweighted_loss = _call_loss_fn(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/canned/head.py\u001b[0m in \u001b[0;36m_label_ids\u001b[0;34m(self, labels)\u001b[0m\n\u001b[1;32m    657\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_integer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m         raise ValueError('Labels dtype should be integer. Instead got {}.'.\n\u001b[0;32m--> 659\u001b[0;31m                          format(labels.dtype))\n\u001b[0m\u001b[1;32m    660\u001b[0m       \u001b[0mlabel_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Labels dtype should be integer. Instead got <dtype: 'string'>."
     ]
    }
   ],
   "source": [
    "# train the network on the training set\n",
    "dnn.train(input_fn=lambda: training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- K-fold splitting of the data\n",
    "- Testing out which epoch and batch sizes are optimal\n",
    "- Graph accuracy over epoch count of training set and test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
