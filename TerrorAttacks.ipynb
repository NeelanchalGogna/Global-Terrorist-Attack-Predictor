{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Terrorist Attacks\n",
    "## Objective\n",
    "To build a classifier that can predict the group responsible for individual terrorist attacks around the world.\n",
    "## Introduction\n",
    "The classifier is trained using the [Global Terrorism Database (GTD)](http://apps.start.umd.edu/gtd/downloads/dataset/Codebook.pdf). The dataset consists of nearly 200,000 terrorist attacks including bombings, assassinations, and kidnappings that have occured since 1970. Each of these attacks includes information on over 45 variables including location, type of weapon used, and nationality of the perpetrator.\n",
    "\n",
    "This classifer is focused on using information about an attack to predict the responsible group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "Before training, the data must be prepared by selecting certain features to remove or augment. The following features from the dataset are kept (reference the [GTD](http://apps.start.umd.edu/gtd/downloads/dataset/Codebook.pdf) for more information on each feature):\n",
    "- <b>Numerical Features (Bucketized)</b><br/>\n",
    "nkill, nkillus, nkillter, nwound, nwoundus, nperps\n",
    "\n",
    "- <b>Categorical Features</b><br/>\n",
    "crit1, crit2, crit3, attacktype1, attacktype2, attacktype3, weaptype1, weapsubtype1, weaptype2, weapsubtype2, weaptype3, weapsubtype3, weaptype4, weapsubtype4, natlty1, natlty2, natlty3, targtype1, targtype2, targtype3, claimed, doubtterr, country, multiple, success, suicide<br/>\n",
    "\n",
    "- <b>Label</b><br/>\n",
    "gnome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make necessary imports\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global variables\n",
    "columns_to_use = ['country', 'crit1', 'crit2', 'crit3', 'doubtterr', 'multiple', 'success', 'suicide', 'attacktype1', 'attacktype2', 'attacktype3', 'targtype1', 'natlty1', 'targtype2', 'natlty2', 'targtype3', 'natlty3', 'nperps', 'claimed', 'weaptype1', 'weapsubtype1', 'weaptype2', 'weapsubtype2', 'weaptype3', 'weapsubtype3', 'weaptype4', 'weapsubtype4', 'nkill', 'nkillus', 'nkillter', 'nwound', 'nwoundus', 'gname']\n",
    "input_file = 'data/gtd1993_0617dist.xlsx'\n",
    "csv_file = 'data/gtd1993_0617dist.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the excel data file to a csv\n",
    "wb = xlrd.open_workbook(input_file)\n",
    "sh = wb.sheets()[0]\n",
    "output_csv = open(csv_file, 'w')\n",
    "wr = csv.writer(output_csv, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "for rownum in range(sh.nrows):\n",
    "    wr.writerow(sh.row_values(rownum))\n",
    "\n",
    "output_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Deep Neural Network\n",
    "Now the neural network must be constructed. It is a deep neural network that uses a mix of numerical, bucketized columns and categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the numerical columns\n",
    "num_killed = tf.feature_column.numeric_column(key='nkill')\n",
    "num_wounded = tf.feature_column.numeric_column(key='nwound')\n",
    "num_us_killed = tf.feature_column.numeric_column(key='nkillus')\n",
    "num_us_wounded = tf.feature_column.numeric_column(key='nwoundus')\n",
    "num_perps = tf.feature_column.numeric_column(key='nperps')\n",
    "num_perps_killed = tf.feature_column.numeric_column(key='nkillter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucketize each of the numerical columns\n",
    "num_killed = tf.feature_column.bucketized_column(source_column=num_killed, boundaries=[5, 25, 50])\n",
    "num_wounded = tf.feature_column.bucketized_column(source_column=num_wounded, boundaries=[5, 25, 50])\n",
    "num_us_killed = tf.feature_column.bucketized_column(source_column=num_us_killed, boundaries=[5, 25, 50])\n",
    "num_us_wounded = tf.feature_column.bucketized_column(source_column=num_us_wounded, boundaries=[5, 25, 50])\n",
    "num_perps = tf.feature_column.bucketized_column(source_column=num_perps, boundaries=[1, 3])\n",
    "num_perp_killed = tf.feature_column.bucketized_column(source_column=num_perps_killed, boundaries=[1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the categorical columns\n",
    "terror_criteria1 = tf.feature_column.categorical_column_with_identity(key='crit1', num_buckets=2)\n",
    "terror_criteria2 = tf.feature_column.categorical_column_with_identity(key='crit2', num_buckets=2)\n",
    "terror_criteria3 = tf.feature_column.categorical_column_with_identity(key='crit3', num_buckets=2)\n",
    "attack_type1 = tf.feature_column.categorical_column_with_identity(key='attacktype1', num_buckets=9)\n",
    "attack_type2 = tf.feature_column.categorical_column_with_identity(key='attacktype2', num_buckets=9)\n",
    "attack_type3 = tf.feature_column.categorical_column_with_identity(key='attacktype3', num_buckets=9)\n",
    "weapon_type1 = tf.feature_column.categorical_column_with_identity(key='weaptype1', num_buckets=13)\n",
    "weapon_subtype1 = tf.feature_column.categorical_column_with_identity(key='weapsubtype1', num_buckets=30)\n",
    "weapon_type2 = tf.feature_column.categorical_column_with_identity(key='weaptype2', num_buckets=13)\n",
    "weapon_subtype2 = tf.feature_column.categorical_column_with_identity(key='weapsubtype2', num_buckets=30)\n",
    "weapon_type3 = tf.feature_column.categorical_column_with_identity(key='weaptype3', num_buckets=13)\n",
    "weapon_subtype3 = tf.feature_column.categorical_column_with_identity(key='weapsubtype3', num_buckets=30)\n",
    "weapon_type4 = tf.feature_column.categorical_column_with_identity(key='weaptype4', num_buckets=13)\n",
    "weapon_subtype4 = tf.feature_column.categorical_column_with_identity(key='weapsubtype4', num_buckets=30)\n",
    "target_nationality1 = tf.feature_column.categorical_column_with_identity(key='natlty1', num_buckets=10004)\n",
    "target_nationality2 = tf.feature_column.categorical_column_with_identity(key='natlty2', num_buckets=10004)\n",
    "target_nationality3 = tf.feature_column.categorical_column_with_identity(key='natlty3', num_buckets=10004)\n",
    "target_type1 = tf.feature_column.categorical_column_with_identity(key='targtype1', num_buckets=22)\n",
    "target_type2 = tf.feature_column.categorical_column_with_identity(key='targtype2', num_buckets=22)\n",
    "target_type3 = tf.feature_column.categorical_column_with_identity(key='targtype3', num_buckets=22)\n",
    "responsibility_claimed = tf.feature_column.categorical_column_with_identity(key='claimed', num_buckets=2)\n",
    "terrorism_doubt = tf.feature_column.categorical_column_with_identity(key='doubtterr', num_buckets=2)\n",
    "country_occurred = tf.feature_column.categorical_column_with_identity(key='country', num_buckets=1004)\n",
    "multiple_incidents = tf.feature_column.categorical_column_with_identity(key='multiple', num_buckets=2)\n",
    "was_successful = tf.feature_column.categorical_column_with_identity(key='success', num_buckets=2)\n",
    "was_suicide = tf.feature_column.categorical_column_with_identity(key='suicide', num_buckets=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert each of the categorical columns to indicator functions\n",
    "terror_criteria1 = tf.feature_column.indicator_column(terror_criteria1)\n",
    "terror_criteria2 = tf.feature_column.indicator_column(terror_criteria2)\n",
    "terror_criteria3 = tf.feature_column.indicator_column(terror_criteria3)\n",
    "attack_type1 = tf.feature_column.indicator_column(attack_type1)\n",
    "attack_type2 = tf.feature_column.indicator_column(attack_type2)\n",
    "attack_type3 = tf.feature_column.indicator_column(attack_type3)\n",
    "weapon_type1 = tf.feature_column.indicator_column(weapon_type1)\n",
    "weapon_subtype1 = tf.feature_column.indicator_column(weapon_subtype1)\n",
    "weapon_type2 = tf.feature_column.indicator_column(weapon_type2)\n",
    "weapon_subtype2 = tf.feature_column.indicator_column(weapon_subtype2)\n",
    "weapon_type3 = tf.feature_column.indicator_column(weapon_type3)\n",
    "weapon_subtype3 = tf.feature_column.indicator_column(weapon_subtype3)\n",
    "weapon_type4 = tf.feature_column.indicator_column(weapon_type4)\n",
    "weapon_subtype4 = tf.feature_column.indicator_column(weapon_subtype4)\n",
    "target_nationality1 = tf.feature_column.indicator_column(target_nationality1)\n",
    "target_nationality2 = tf.feature_column.indicator_column(target_nationality2)\n",
    "target_nationality3 = tf.feature_column.indicator_column(target_nationality3)\n",
    "target_type1 = tf.feature_column.indicator_column(target_type1)\n",
    "target_type2 = tf.feature_column.indicator_column(target_type2)\n",
    "target_type3 = tf.feature_column.indicator_column(target_type3)\n",
    "responsibility_claimed = tf.feature_column.indicator_column(responsibility_claimed)\n",
    "terrorism_doubt = tf.feature_column.indicator_column(terrorism_doubt)\n",
    "country_occurred = tf.feature_column.indicator_column(country_occurred)\n",
    "multiple_incidents = tf.feature_column.indicator_column(multiple_incidents)\n",
    "was_successful = tf.feature_column.indicator_column(was_successful)\n",
    "was_suicide = tf.feature_column.indicator_column(was_suicide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a list of each of the deep columns\n",
    "deep_columns = [\n",
    "    num_killed,\n",
    "    num_us_killed,\n",
    "    num_us_wounded,\n",
    "    num_perps,\n",
    "    num_perp_killed,\n",
    "    terror_criteria1,\n",
    "    terror_criteria2,\n",
    "    terror_criteria3,\n",
    "    attack_type1,\n",
    "    attack_type2,\n",
    "    attack_type3,\n",
    "    weapon_type1,\n",
    "    weapon_subtype1,\n",
    "    weapon_type2,\n",
    "    weapon_subtype2,\n",
    "    weapon_type3,\n",
    "    weapon_subtype3,\n",
    "    weapon_type4,\n",
    "    weapon_subtype4,\n",
    "    target_nationality1,\n",
    "    target_nationality2,\n",
    "    target_nationality3,\n",
    "    target_type1,\n",
    "    target_type2,\n",
    "    target_type3,\n",
    "    responsibility_claimed,\n",
    "    terrorism_doubt,\n",
    "    country_occurred,\n",
    "    multiple_incidents,\n",
    "    was_successful,\n",
    "    was_suicide\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/xm/l9l7xyqx2qs1f_7yfkshv9v40000gn/T/tmpb48pw6mh\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/xm/l9l7xyqx2qs1f_7yfkshv9v40000gn/T/tmpb48pw6mh', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x12088d470>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# initializing the DNN\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "    n_classes=100,\n",
    "    feature_columns=deep_columns,\n",
    "    hidden_units=[100, 50],\n",
    "    activation_fn=tf.nn.relu,\n",
    "    dropout=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "Now the network must be trained on the GTD data from 1993."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to load in the training data\n",
    "def load_data(label_name):\n",
    "    train = pd.read_csv(csv_file, usecols=columns_to_use)\n",
    "    train_x, train_y = train, train.pop(label_name)\n",
    "    \n",
    "    # replace all NaN values with -1\n",
    "    train_x.fillna(-1, inplace=True)\n",
    "    \n",
    "    # convert the features into a dictionary of tensors\n",
    "    train_final_x = {}\n",
    "    for key in train_x:\n",
    "        train_final_x[key] = tf.convert_to_tensor(train_x[key].astype(int))\n",
    "    \n",
    "    # convert the labels into a tensor of integers\n",
    "    tensor_y = tf.convert_to_tensor(train_y)\n",
    "    train_final_y = tf.string_to_number(tensor_y, out_type=tf.int32)\n",
    "    \n",
    "    # return the features and labels as tensors\n",
    "    return train_final_x, train_final_y\n",
    "    \n",
    "# retrieve the training data\n",
    "training_data = load_data(label_name='gname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Passed Tensor(\"dnn/head/weighted_loss/Sum:0\", shape=(), dtype=float32) should have graph attribute that is equal to current graph <tensorflow.python.framework.ops.Graph object at 0x120898c18>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f207c4e0961e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train the network on the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    898\u001b[0m           \u001b[0msave_summaries_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_summary_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m           \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m           log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\n\u001b[0m\u001b[1;32m    901\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mMonitoredTrainingSession\u001b[0;34m(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0mall_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m   return MonitoredSession(session_creator=session_creator, hooks=all_hooks,\n\u001b[0;32m--> 384\u001b[0;31m                           stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    793\u001b[0m     super(MonitoredSession, self).__init__(\n\u001b[1;32m    794\u001b[0m         \u001b[0msession_creator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m       \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m     \u001b[0;31m# Create the session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     self._coordinated_creator = self._CoordinatedSessionCreator(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;31m# Convert names to tensors if given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     self._current_tensors = {tag: _as_graph_element(tensor)\n\u001b[0;32m--> 230\u001b[0;31m                              for (tag, tensor) in self._tensors.items()}\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbefore_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;31m# Convert names to tensors if given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     self._current_tensors = {tag: _as_graph_element(tensor)\n\u001b[0;32m--> 230\u001b[0;31m                              for (tag, tensor) in self._tensors.items()}\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbefore_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py\u001b[0m in \u001b[0;36m_as_graph_element\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    908\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"graph\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       raise ValueError(\"Passed %s should have graph attribute that is equal \"\n\u001b[0;32m--> 910\u001b[0;31m                        \"to current graph %s.\" % (obj, graph))\n\u001b[0m\u001b[1;32m    911\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m\":\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Passed Tensor(\"dnn/head/weighted_loss/Sum:0\", shape=(), dtype=float32) should have graph attribute that is equal to current graph <tensorflow.python.framework.ops.Graph object at 0x120898c18>."
     ]
    }
   ],
   "source": [
    "# train the network on the training set\n",
    "dnn.train(input_fn=lambda: training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- K-fold splitting of the data\n",
    "- Testing out which epoch and batch sizes are optimal\n",
    "- Graph accuracy over epoch count of training set and test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
